{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "630de7ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Databricks notebook source\n",
    "# DBTITLE 1,Library Imports\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# DBTITLE 1,Function to get the DataFrame based on specified location and format\n",
    "\"\"\"\n",
    "function: getDataFrame\n",
    "  params (param name[param type])\n",
    "    1) format[str] - defines the format of the DataFrame source (e.g. parquet/csv)\n",
    "    2) header[str] = should include header or not (e.g. false)\n",
    "    3) delimiter[str] = column delimiter to split the columns (e.g. \\t)\n",
    "    4) location[str] = location of the file to generate the DataFrame for (e.g. /mnt/abidatasummaryread/Intake/MSSales/MSFSALSL00/)\n",
    "    5) schema[StructType] = schema of the file mentioned in location param (e.g. output generated by getTableSchema(MSFSALSL00) call)\n",
    "  return DataFrame\n",
    "\"\"\"\n",
    "\n",
    "def getDataFrame(format: str, header: str, delimiter: str, location: str, schema: StructType) -> DataFrame:\n",
    "  df = spark.read.format(format).option(\"header\",header).schema(schema).option(\"delimiter\",delimiter).load(location)\n",
    "  return df\n",
    "\n",
    "\n",
    "def getDataFrameParquet(location: str) -> DataFrame:\n",
    "  df = spark.read.parquet(location)\n",
    "  return df\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# DBTITLE 1,Function to get the FYM{*}/FYMxx based on processing needs\n",
    "\"\"\"\n",
    "function: getFiscalYearRange\n",
    "  params (param name[param type])\n",
    "    1) param[str] - parameter to get the fiscal year ranage (e.g. d:FY21)\n",
    "    first character (d/f/r) before : in d:FY21 defines if processing is (d)daily/f(FME)/r(restatement) \n",
    "    value after : in d:FY21 defines for how-many/what years needs to be considered for processing\n",
    "    d: is special case and goes from CM-1 to M12\n",
    "    f/r behave almost similar and are placeholders for any special scenario that may come\n",
    "    f/r:FY20#FY21: process data for two complete years together. Can be used for any number of years.\n",
    "        Performance of your notebook may vary based on these parameters\n",
    "  return DataFrame\n",
    "\"\"\"\n",
    "\n",
    "def getFiscalYearRange(param:str) -> DataFrame:\n",
    "  fy = param.replace(\"d:\",\"\").replace(\"f:\",\"\").replace(\"r:\",\"\")\n",
    "  op = param[0:1]\n",
    "  if op == \"d\":\n",
    "    sql = \"\"\"WITH CTE_Date\n",
    "    AS\n",
    "    (\n",
    "        SELECT DISTINCT\n",
    "            FiscalYearName\n",
    "            , FiscalMonthName\n",
    "            , FiscalMonth\n",
    "            , REPLACE(CASE WHEN RelativeMonth = 'CM' THEN 'CM0' ELSE RelativeMonth END, 'CM', '') AS RelativeMonth\n",
    "        FROM\n",
    "           delta.`/mnt/abidatasummaryblobread/ConformedDataProd/ConformedDelta/DIM_Date/` AS d\n",
    "        WHERE\n",
    "            FiscalYearName = '\"\"\" + fy + \"\"\"'\n",
    "    )\n",
    "    SELECT\n",
    "        DISTINCT \n",
    "        LEFT(FiscalMonth, 4) AS FiscalYear\n",
    "        , concat(RIGHT(FiscalMonth, 3),'{*}') AS FiscalMonth\n",
    "        , FiscalMonth AS NameSuffix\n",
    "    FROM CTE_Date\n",
    "    WHERE RelativeMonth >= CASE WHEN date_format(current_date(), 'dd')  > 2 THEN -1 ELSE -2 END  \n",
    "    \"\"\"\n",
    "  else:\n",
    "    sql = \"\"\"\n",
    "    WITH CTE_Years\n",
    "    AS\n",
    "    (\n",
    "      SELECT explode(split('\"\"\" + fy + \"\"\"','#')) AS Year\n",
    "    )\n",
    "    SELECT TRIM(Year) AS FiscalYear, 'M{*}' AS FiscalMonth, TRIM(Year) AS NameSuffix\n",
    "    FROM CTE_Years\n",
    "    \"\"\"\n",
    "  df = spark.sql(sql)\n",
    "  return df\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# DBTITLE 1,Function to get the schema (StructType) based on active DYN Columns\n",
    "\"\"\"\n",
    "function: getTableSchema\n",
    "  params (param name[param type])\n",
    "    1) table[str] - table name of for which schema is required (e.g. MSFSALSL00)\n",
    "        The name of the table provided in the parameter MUST match with SourceTableName column of\n",
    "        [ABIVMPROSQL06\\SUMMARY].STARLIGHT_00_ETL.dbo.DYN_Table table.\n",
    "        The parquet at (/mnt/abidatasummary/Intake/dbo/DYNTableColumn/DYNTableColumn.parquet) is updated based on \n",
    "        MSSales intake under 01MSSalesADLSUpload schedule. Only BUS_DoNoUse = 0 and NotInSource = 0 columns are available.\n",
    "  return StructType\n",
    "    1) The ouput StructType is a JSON based schema translated to Spark-SQL compatible data-types based on trasnlation in the following function\n",
    "        The field structure is {name:, type:, nullable:, metadata:}\n",
    "\"\"\"\n",
    "\n",
    "def getTableSchema(table:str) -> StructType:\n",
    "  sql = \"\"\"\n",
    "  WITH CTE_Columns AS\n",
    "  (\n",
    "    SELECT\n",
    "      DestinationColumnName\n",
    "      , lower(SourceColumnDataType) AS SourceColumnDataType\n",
    "      , case when SourceColumnIsNullable = 1 THEN 'True' else 'False' end Nullable\n",
    "      , ColumnOrder\n",
    "    FROM\n",
    "      parquet.`/mnt/abidatasummary/Intake/dbo/DYNTableColumn/DYNTableColumn.parquet`\n",
    "    WHERE lower(SourceTableName) = lower('\"\"\" + table + \"\"\"')\n",
    "  )\n",
    "  SELECT\n",
    "    DestinationColumnName\n",
    "    , CASE \n",
    "        WHEN SourceColumnDataType = 'int' AND DestinationColumnName = 'LicenseTransactionItemId' THEN 'long'\n",
    "        WHEN SourceColumnDataType = 'int' AND DestinationColumnName <> 'LicenseTransactionItemId' THEN 'integer'\n",
    "        WHEN SourceColumnDataType = 'tinyint' THEN 'integer'\n",
    "        WHEN SourceColumnDataType = 'bigint' THEN 'long'\n",
    "        WHEN SourceColumnDataType = 'smallint' THEN 'integer'\n",
    "        WHEN SourceColumnDataType = 'numeric' THEN 'decimal(19,4)'\n",
    "        WHEN SourceColumnDataType = 'varchar' THEN 'string'\n",
    "        WHEN SourceColumnDataType = 'nvarchar' THEN 'string'\n",
    "        WHEN SourceColumnDataType = 'char' THEN 'string'\n",
    "        ELSE 'string'\n",
    "      END DataType\n",
    "    , Nullable\n",
    "  FROM\n",
    "    CTE_Columns\n",
    "  ORDER BY ColumnOrder\n",
    "  \"\"\"\n",
    "  columnDF = spark.sql(sql)\n",
    "  schema = \"\"\n",
    "  for row in columnDF.rdd.collect():\n",
    "    schema = schema + \"{'name': '\" + row.DestinationColumnName + \"', 'type': '\" + row.DataType + \"', 'nullable':\"  + row.Nullable + \", 'metadata': {}}\"\n",
    "  \n",
    "  schema =  \"{'type': 'struct', 'fields': [\" + schema.replace(\"{}}{\",\"{}},\\n{\") + \"]}\"\n",
    "  schema = StructType.fromJson(eval(schema))\n",
    "  return schema\n",
    "\n",
    "# COMMAND ----------\n",
    "  %sql\n",
    "CREATE OR REPLACE TEMP VIEW vwMDSToBRE\n",
    "AS\n",
    "SELECT * FROM parquet.`/mnt/abidatadetail/Intake/MDS/MWDetail/MDSToBRE`;\n",
    "\n",
    "CREATE OR REPLACE TEMP VIEW MetricName_Tbl\n",
    "AS\n",
    "WITH CTE AS \n",
    "   (SELECT MetricGroup,MetricName,MIN(Id) Id \n",
    "     FROM vwMDSToBRE GROUP BY  MetricGroup,MetricName)\n",
    "SELECT MetricGroup,\n",
    "       MetricName,\n",
    "       Row_Number() over(Partition BY MetricGroup Order by Id) Rn  \n",
    "  FROM CTE;\n",
    "\n",
    "  # COMMAND ----------\n",
    "  %scala\n",
    "def MDSBre(MetricGroup:String) : String = \n",
    "{ \n",
    "var x = 1;\n",
    "var mdsbre_sql = \"CASE \";\n",
    "var WhereClause = \"\";\n",
    "var MetricName_Cnt = spark.sql(\"SELECT COUNT(1) FROM MetricName_Tbl WHERE MetricGroup = '\"+MetricGroup+\"'\").first.get(0).toString(); \n",
    "var DefaultValue = spark.sql(\"SELECT DISTINCT DefaultValue FROM vwMDSToBRE WHERE MetricGroup = '\"+MetricGroup+\"'\").first.get(0).toString(); \n",
    "var y = MetricName_Cnt.toInt\n",
    "while(x <= y)\n",
    "{          \n",
    "          var WhereClause1=\"WHEN \";\n",
    "          var MetricName = spark.sql(\"SELECT MetricName FROM MetricName_Tbl WHERE RN=\"+x+\" AND MetricGroup = '\"+MetricGroup+\"'\").first.get(0).toString();\n",
    "          var M365_MinId = spark.sql(\"SELECT MIN(Id) MAXID FROM vwMDSToBRE WHERE MetricName = '\" + MetricName+\"' AND MetricGroup = '\"+MetricGroup+\"'\").first.get(0).toString(); \n",
    "          var M365_MaxId = spark.sql(\"SELECT MAX(Id) MINID FROM vwMDSToBRE WHERE MetricName = '\" + MetricName+\"' AND MetricGroup = '\"+MetricGroup+\"'\").first.get(0).toString(); \n",
    "          var Result = spark.sql(\"SELECT Result FROM vwMDSToBRE WHERE ID=\"+M365_MaxId + \" AND MetricName = '\" + MetricName+\"' AND MetricGroup = '\"+MetricGroup+\"'\").first.get(0).toString(); \n",
    "          var j = M365_MaxId.toInt\n",
    "          var i = M365_MinId.toInt\n",
    "\n",
    "          while(i <= j)\n",
    "            {\n",
    "                 var WhereClause2 = spark.sql(\"\"\"SELECT CONCAT(FieldName , \" \", Operator , \" \" , FilterCondition , \" \"  , COALESCE(LINK,\"\") )  FROM vwMDSToBRE WHERE ID = \"\"\"+i+ \" AND MetricName = '\" + MetricName+\"' AND MetricGroup = '\"+MetricGroup+\"'\").first.get(0).toString()\n",
    "                 WhereClause1 = WhereClause1 + \" \" +WhereClause2 \n",
    "             i = i+1\n",
    "\n",
    "            }\n",
    "         WhereClause = WhereClause + \" \" +WhereClause1 + \" THEN '\" +Result + \"'\"\n",
    "  x = x+1\n",
    "\n",
    "}\n",
    "mdsbre_sql = mdsbre_sql + WhereClause +  \" ELSE '\" + DefaultValue + \"' END\"\n",
    "return mdsbre_sql\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dbf3b04c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xref table not zero-indexed. ID numbers for objects will be corrected.\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'C:\\\\Users\\\\Python_learning\\\\\\\\lm-tcfd-report-2021-1.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 26>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     18\u001b[0m text\u001b[38;5;241m=\u001b[39mpageobj\u001b[38;5;241m.\u001b[39mextractText()\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m#save the extracted data from pdf to a txt file\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m#we will use file handling here\u001b[39;00m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m#dont forget to put r before you put the file path\u001b[39;00m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m#go to the file location copy the path by right clicking on the file\u001b[39;00m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m#click properties and copy the location path and paste it here.\u001b[39;00m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m#put \"\\\\your_txtfilename\"\u001b[39;00m\n\u001b[1;32m---> 26\u001b[0m file1\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mC:\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mUsers\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mPython_learning\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;124;43mlm-tcfd-report-2021-1.txt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     27\u001b[0m file1\u001b[38;5;241m.\u001b[39mwritelines(text)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'C:\\\\Users\\\\Python_learning\\\\\\\\lm-tcfd-report-2021-1.txt'"
     ]
    }
   ],
   "source": [
    "import PyPDF2\n",
    "import pandas as pd\n",
    "#create file object variable\n",
    "#opening method will be rb\n",
    "pdffileobj=open(r'C:\\Users\\Python_learning\\\\lm-tcfd-report-2021-1.pdf','rb')\n",
    " \n",
    "#create reader variable that will read the pdffileobj\n",
    "pdfreader=PyPDF2.PdfFileReader(pdffileobj)\n",
    " \n",
    "#This will store the number of pages of this pdf file\n",
    "x=pdfreader.numPages\n",
    " \n",
    "#create a variable that will select the selected number of pages\n",
    "pageobj=pdfreader.getPage(x-1)\n",
    " \n",
    "#(x+1) because python indentation starts with 0.\n",
    "#create text variable which will store all text datafrom pdf file\n",
    "text=pageobj.extractText()\n",
    " \n",
    "#save the extracted data from pdf to a txt file\n",
    "#we will use file handling here\n",
    "#dont forget to put r before you put the file path\n",
    "#go to the file location copy the path by right clicking on the file\n",
    "#click properties and copy the location path and paste it here.\n",
    "#put \"\\\\your_txtfilename\"\n",
    "file1=open(r\"C:\\Users\\Python_learning\\\\lm-tcfd-report-2021-1.txt\")\n",
    "file1.writelines(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c117a52a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import PyPDF2\n",
    "from PyPDF2 import PdfMerger, PdfReader, PdfWriter\n",
    "import pandas as pd\n",
    "import pikepdf\n",
    "pdf = pikepdf.Pdf.open(r'C:/Users/v-shivampa/omy.pdf')\n",
    "#create file object variable\n",
    "#opening method will be rb\n",
    "#pdffileobj1=open(r'C:/Users/v-shivampa/omy.pdf','rb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "11cae675",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'PyPDF2._reader.PdfReader'>\n"
     ]
    }
   ],
   "source": [
    "#pdfreader=PyPDF2.PdfFileReader(pdffileobj1)\n",
    "print(PdfReader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5ff697c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x=pdfreader.numPages\n",
    "#text=pdfreader.extractText()\n",
    "#text\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "819f8e6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pageobj=pdfreader.getPage(x-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "0c03382d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2021 TASK FORCE ON CLIMATE-RELATED FINANCIAL DISCLOSURES REPORT31\\n Contact us \\nFor questions or comments regarding this report,  please contact Sustainability@LibertyMutual.com.\\nCautionary statement regarding this reportThis report has been prepared solely for informational purposes from sources understood by the Company to be reliable at the time included in this report. Liberty Mutual Group (the Company) does not guarantee the accuracy, completeness, timeliness, or availability of the contents of this report. The Company is not responsible for any errors or omissions, regardless of the cause, for the results obtained from the use of the contents of this report. In no event shall the Company be liable to any party for any direct, indirect, incidental, exemplary, compensatory, punitive, special, or consequential damages, costs, expenses, legal fees, or losses (including, without limitation, lost income or lost profits and opportunity costs or losses caused by negligence) in connection with any use of the contents of this report even if advised of the possibility of such damages. The Company’s opinions, quotes, and analyses are statements of opinion as of the date they are expressed and not statements of fact or recommendations to purchase, hold, or sell any securities or to make any investment or insurance-related decisions and do not address the suitability of any security or insurance policy. The contents of this report should not be relied on and is not a substitute for the skill, judgment and experience of the user, its management, employees, advisors, and/or clients when making investment, insurance-related, and other business decisions. This report contains forward looking statements that are intended to enhance the reader’s ability to assess the Company’s future financial and business performance. Forward looking statements include, but are not limited to, statements that represent the Company’s beliefs concerning future operations, strategies, financial results, or other developments, and contain words and phrases such as “may, ” “expects, ” “should, ” “believes, ” “anticipates, ” “estimates, ” “intends” or similar expressions. Because these forward-looking statements are based on estimates and assumptions that are subject to significant business, environmental, economic, and competitive uncertainties, many of which are beyond the Company’s control or are subject to change, actual results could be materially different. The Company’s forward-looking statements speak only as of the date of this report or as of the date they are made and should be regarded solely as the Company’s current plans, estimates and beliefs. The Company assumes no obligation to update these forward-looking statements or the contents of this report following publication in any form or format. For a discussion of the Company’s financial information, visit the Company’s Investor Relations website at www.libertymutualgroup.com/investors.   This report may contain links to other Internet sites and may frame material from other Internet sites. Such links or frames are not endorsements of any products or services in such sites, and no information in such sites has been endorsed or approved by the Company.Except where noted, the information covered in this report highlights our performance and initiatives in fiscal  year 2021. The inclusion of information in this report should not be construed as a characterization regarding the materiality  or financial impact (or potential impact) of that information.'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text=pageobj.extractText()\n",
    "text"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a5cc00c3",
   "metadata": {},
   "source": [
    "#create text variable which will store all text datafrom pdf file\n",
    "text=pageobj.extractText()\n",
    "text"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
